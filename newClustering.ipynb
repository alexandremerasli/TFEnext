{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/skbio/util/_testing.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as pdt\n"
     ]
    }
   ],
   "source": [
    "# Useful packages\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Work with Python array and graphs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Cluster tendency\n",
    "from pyclustertend import hopkins\n",
    "\n",
    "# Mapping\n",
    "from sklearn.metrics import pairwise_distances, accuracy_score\n",
    "from coclust.evaluation.external import accuracy\n",
    "from skbio.stats.ordination import pcoa\n",
    "from multiview.mvmds import mvmds\n",
    "\n",
    "# Clustering algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#from KMedoidsPaper import KMedoids\n",
    "\n",
    "# Clustering performance evaluation\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn==0.22.1\n",
      "numpy==1.17.0\n",
      "matplotlib==3.2.1\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISC_EDA = np.genfromtxt(os.path.join('school_EDA12.csv'),delimiter=',')\n",
    "ISC_EDA = np.array(ISC_EDA,dtype='float') # Convert into array\n",
    "\n",
    "ISC_IBI = np.genfromtxt(os.path.join('school_IBI.csv'),delimiter=',')\n",
    "ISC_IBI = np.array(ISC_IBI,dtype='float') # Convert into array\n",
    "\n",
    "allISC = [ISC_EDA,ISC_IBI]\n",
    "\n",
    "nParticipants = ISC_EDA.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC_EDA.shape[1] / nParticipants))\n",
    "ISC_EDA_sh = np.zeros((ISC_EDA.shape[0]*nPeriod,ISC_EDA.shape[0]*nPeriod))\n",
    "nParticipants = ISC_IBI.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC_IBI.shape[1] / nParticipants))\n",
    "ISC_IBI_sh = np.zeros((ISC_IBI.shape[0]*nPeriod,ISC_IBI.shape[0]*nPeriod))\n",
    "\n",
    "allISC_sh = [ISC_EDA_sh,ISC_IBI_sh]\n",
    "\n",
    "#ISC_IBI = ISC_IBI[0:10,0:10]\n",
    "ISC_IBI = ISC_EDA\n",
    "\n",
    "for i in range(len(allISC)):\n",
    "    \n",
    "    ISC = allISC[i]\n",
    "    ISC_sh = allISC_sh[i]\n",
    "    \n",
    "    nParticipants = ISC.shape[0]\n",
    "    nPeriod = int(np.sqrt(ISC.shape[1] / nParticipants))\n",
    "\n",
    "    ISC_sh = np.zeros((ISC.shape[0]*nPeriod,ISC.shape[0]*nPeriod))\n",
    "\n",
    "    for period1 in range(nPeriod):\n",
    "        for period2 in range(nPeriod):\n",
    "            if (period1 == period2):\n",
    "                subISC_EDA = np.nan_to_num(ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)])\n",
    "                subISC_EDA = (subISC_EDA+subISC_EDA.T)/2\n",
    "            else:\n",
    "                subISC_EDA = ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)]\n",
    "            ISC_sh[nParticipants*period1:nParticipants*(period1+1),nParticipants*period2:nParticipants*(period2+1)] = subISC_EDA\n",
    "\n",
    "    ISC_sh = np.nan_to_num(ISC_sh)\n",
    "    ISC_sh = (ISC_sh+ISC_sh.T)/2\n",
    "\n",
    "import csv\n",
    "    \n",
    "with open(os.path.join('test.csv'), 'w') as File:\n",
    "    writer = csv.writer(File,delimiter =',')\n",
    "    writer.writerows(ISC_EDA_sh)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 2D to 4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nPeriod = 7\n",
    "ISC_EDA = np.zeros((ISC_EDA_2D.shape[0],int(ISC_EDA_2D.shape[1]/(nPeriod**2)),nPeriod,nPeriod))\n",
    "for i in range(nPeriod):\n",
    "    ISC_EDA[:,:,i] = ISC_EDA_2D[:,80*i:80*(i+1)]\n",
    "    \n",
    "print(ISC_EDA.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''nPeriod = 2\n",
    "ISC_EDA = np.reshape(ISC_EDA,(ISC_EDA.shape[0],int(ISC_EDA.shape[1]/(nPeriod**2)),nPeriod,nPeriod))\n",
    "ISC_IBI = np.reshape(ISC_IBI,(ISC_IBI.shape[0],int(ISC_IBI.shape[1]/(nPeriod**2)),nPeriod,nPeriod))\n",
    "\n",
    "print(ISC_EDA.shape)\n",
    "print(ISC_IBI.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing specific to these matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Symetrical matrices\n",
    "ISC_EDA = np.nan_to_num(ISC_EDA)\n",
    "ISC_IBI = np.nan_to_num(ISC_IBI)\n",
    "ISC_EDA = (ISC_EDA+np.swapaxes(ISC_EDA,0,1))/2\n",
    "ISC_IBI = (ISC_IBI+np.swapaxes(ISC_IBI,0,1))/2\n",
    "\n",
    "print(ISC_EDA[:4,:4,0,0])\n",
    "print(ISC_IBI[:4,:4,0,0])\n",
    "\n",
    "#ISC_EDA = ISC_EDA[:,:,0,0]\n",
    "#ISC_IBI = ISC_IBI[:,:,0,0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ISC_EDA = np.swapaxes(ISC_EDA,1,2)\n",
    "ISC_EDA = np.flip(ISC_EDA,3)\n",
    "print(ISC_EDA.shape[0])\n",
    "ISC_EDA = np.reshape(ISC_EDA,((ISC_EDA.shape[0]*ISC_EDA.shape[1],ISC_EDA.shape[0]*ISC_EDA.shape[1])))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 12 4\n",
      "1 12 15\n",
      "[4, 15]\n",
      "[[ 1.00000000e+00  6.37051073e-04 -2.61231164e-03  5.29814469e-02\n",
      "   6.65845507e-02  1.98344924e-02  3.99448108e-02 -2.81809301e-02\n",
      "   2.74517993e-03 -5.10090644e-04 -1.55923997e-02  3.65620732e-02\n",
      "   4.90872538e-03  2.49558439e-02  4.48140993e-02  2.43796086e-02\n",
      "   5.06812548e-02  1.64625992e-02  2.66983402e-02 -3.57981629e-03\n",
      "   9.03370597e-03  2.07368315e-02]\n",
      " [ 6.37051073e-04  1.00000000e+00  4.28282390e-02  2.94378457e-02\n",
      "  -7.29108494e-03  2.08548494e-02  2.15766473e-02  1.22437371e-02\n",
      "   2.37337624e-02 -2.23515011e-02  2.83020822e-02  2.00132953e-02\n",
      "   2.25733339e-02  4.65858224e-02  2.72202158e-02  1.75468155e-02\n",
      "   4.59557816e-02  4.18915226e-02  5.07778813e-02 -1.00154837e-04\n",
      "   5.31188130e-03  2.91983717e-02]\n",
      " [-2.61231164e-03  4.28282390e-02  1.00000000e+00  1.44376243e-02\n",
      "   1.59513263e-02  2.71206188e-02  4.08477407e-02  4.74072908e-02\n",
      "   2.32658917e-02  4.25095435e-03  3.78597466e-02  3.82443523e-03\n",
      "  -6.39143054e-03  2.02213082e-02  7.85733397e-03  9.91840490e-03\n",
      "   1.91933431e-02  1.19770206e-02  2.17407887e-02  4.83161702e-02\n",
      "  -8.70886098e-05  4.23188694e-02]\n",
      " [ 5.29814469e-02  2.94378457e-02  1.44376243e-02  1.00000000e+00\n",
      "   3.86634522e-02  2.67509320e-02  1.98488536e-02  1.94083678e-02\n",
      "   2.20164900e-02  2.37586559e-02  2.31907796e-02  4.22452913e-02\n",
      "   3.19403896e-02  1.56019060e-02  2.30670113e-02 -1.82971832e-02\n",
      "   4.25369017e-03  4.99936211e-02  2.58202628e-02  1.70302835e-02\n",
      "  -1.13381911e-02  4.48199899e-02]\n",
      " [ 6.65845507e-02 -7.29108494e-03  1.59513263e-02  3.86634522e-02\n",
      "   1.00000000e+00  2.65419541e-02  1.19534156e-02  4.31664665e-03\n",
      "   1.49567324e-02  4.19905446e-02  1.65251306e-02  2.98108050e-02\n",
      "  -4.07835339e-03  5.32755413e-02 -2.13812746e-02  1.75353821e-02\n",
      "   2.62916927e-02  4.67697353e-02  5.55360205e-02 -2.92729579e-03\n",
      "   5.47509212e-03 -1.84536481e-02]\n",
      " [ 1.98344924e-02  2.08548494e-02  2.71206188e-02  2.67509320e-02\n",
      "   2.65419541e-02  1.00000000e+00  3.26610354e-02  2.33592232e-02\n",
      "   3.97179125e-02  1.01078871e-02  1.11373735e-02 -1.82427845e-02\n",
      "   2.35374402e-02  4.18496014e-02  4.31471193e-02  1.18937152e-02\n",
      "   7.54375789e-03  2.43991608e-02  4.14951645e-02  8.02137193e-02\n",
      "   9.86710561e-03  2.81362333e-02]\n",
      " [ 3.99448108e-02  2.15766473e-02  4.08477407e-02  1.98488536e-02\n",
      "   1.19534156e-02  3.26610354e-02  1.00000000e+00  6.36993011e-02\n",
      "   3.35015503e-02  1.95278850e-03  5.39418847e-02 -1.78047927e-02\n",
      "   1.77764276e-02  1.38050727e-02  4.31946111e-02  4.85635730e-02\n",
      "   4.16224147e-02  6.34995511e-02  6.75080691e-02  7.14987016e-03\n",
      "   1.51463676e-02  7.21636116e-02]\n",
      " [-2.81809301e-02  1.22437371e-02  4.74072908e-02  1.94083678e-02\n",
      "   4.31664665e-03  2.33592232e-02  6.36993011e-02  1.00000000e+00\n",
      "   4.07894385e-02 -4.53554670e-03  3.55671160e-02 -8.32135379e-03\n",
      "   2.45088278e-02  1.55272283e-02  4.70597883e-02 -3.04582475e-03\n",
      "   5.17798132e-02  2.91672639e-02  5.28106886e-02  1.96930621e-02\n",
      "  -2.78981964e-03  1.14867244e-02]\n",
      " [ 2.74517993e-03  2.37337624e-02  2.32658917e-02  2.20164900e-02\n",
      "   1.49567324e-02  3.97179125e-02  3.35015503e-02  4.07894385e-02\n",
      "   1.00000000e+00  1.95571332e-02  4.22543512e-03  2.07953446e-02\n",
      "   3.98372077e-02  1.86173005e-02  3.08236553e-02  5.19762852e-02\n",
      "   3.43159991e-02 -1.07850186e-02  5.51542664e-02  1.21149985e-02\n",
      "  -6.38882324e-03  2.58360996e-02]\n",
      " [-5.10090644e-04 -2.23515011e-02  4.25095435e-03  2.37586559e-02\n",
      "   4.19905446e-02  1.01078871e-02  1.95278850e-03 -4.53554670e-03\n",
      "   1.95571332e-02  1.00000000e+00  2.76976167e-02 -5.94454617e-03\n",
      "  -1.33499016e-02  7.69328806e-03 -6.31128466e-03  1.85592576e-02\n",
      "  -3.23888087e-02  1.46155798e-02  5.68873843e-03  1.96606278e-02\n",
      "   5.63884112e-03  1.24286913e-02]\n",
      " [-1.55923997e-02  2.83020822e-02  3.78597466e-02  2.31907796e-02\n",
      "   1.65251306e-02  1.11373735e-02  5.39418847e-02  3.55671160e-02\n",
      "   4.22543512e-03  2.76976167e-02  1.00000000e+00  3.14009214e-03\n",
      "   2.78471545e-02 -2.45612219e-02  2.88103014e-02  6.16697666e-03\n",
      "   1.50810054e-02  3.41039779e-02  1.99567235e-02  4.25203771e-03\n",
      "  -2.01475617e-03 -1.77237615e-02]\n",
      " [ 3.65620732e-02  2.00132953e-02  3.82443523e-03  4.22452913e-02\n",
      "   2.98108050e-02 -1.82427845e-02 -1.78047927e-02 -8.32135379e-03\n",
      "   2.07953446e-02 -5.94454617e-03  3.14009214e-03  1.00000000e+00\n",
      "   4.47327529e-02  1.46530119e-03  1.21468371e-02  4.57164378e-02\n",
      "   4.29147807e-02  3.47968096e-02  3.98763004e-02  9.53739270e-03\n",
      "   6.59780259e-02 -1.01023007e-02]\n",
      " [ 4.90872538e-03  2.25733339e-02 -6.39143054e-03  3.19403896e-02\n",
      "  -4.07835339e-03  2.35374402e-02  1.77764276e-02  2.45088278e-02\n",
      "   3.98372077e-02 -1.33499016e-02  2.78471545e-02  4.47327529e-02\n",
      "   1.00000000e+00  1.03790516e-03  2.77095019e-02  4.50046343e-02\n",
      "   4.33658548e-02  4.88964604e-02  1.52287534e-02  2.97460896e-02\n",
      "  -2.11717157e-02  2.39338538e-03]\n",
      " [ 2.49558439e-02  4.65858224e-02  2.02213082e-02  1.56019060e-02\n",
      "   5.32755413e-02  4.18496014e-02  1.38050727e-02  1.55272283e-02\n",
      "   1.86173005e-02  7.69328806e-03 -2.45612219e-02  1.46530119e-03\n",
      "   1.03790516e-03  1.00000000e+00  1.43772837e-02  3.92857423e-02\n",
      "   3.76311600e-02  2.31927456e-02  1.36387678e-02  2.42191770e-02\n",
      "   3.04457647e-02  5.99914448e-03]\n",
      " [ 4.48140993e-02  2.72202158e-02  7.85733397e-03  2.30670113e-02\n",
      "  -2.13812746e-02  4.31471193e-02  4.31946111e-02  4.70597883e-02\n",
      "   3.08236553e-02 -6.31128466e-03  2.88103014e-02  1.21468371e-02\n",
      "   2.77095019e-02  1.43772837e-02  1.00000000e+00  2.29902152e-02\n",
      "   9.04287241e-03  6.05218606e-02  1.11587480e-02  1.52770807e-02\n",
      "  -4.49526854e-03  1.61558954e-02]\n",
      " [ 2.43796086e-02  1.75468155e-02  9.91840490e-03 -1.82971832e-02\n",
      "   1.75353821e-02  1.18937152e-02  4.85635730e-02 -3.04582475e-03\n",
      "   5.19762852e-02  1.85592576e-02  6.16697666e-03  4.57164378e-02\n",
      "   4.50046343e-02  3.92857423e-02  2.29902152e-02  1.00000000e+00\n",
      "   4.70644725e-02  2.05511100e-02  3.09067166e-02  5.74860000e-02\n",
      "  -1.12095432e-02  6.46407448e-04]\n",
      " [ 5.06812548e-02  4.59557816e-02  1.91933431e-02  4.25369017e-03\n",
      "   2.62916927e-02  7.54375789e-03  4.16224147e-02  5.17798132e-02\n",
      "   3.43159991e-02 -3.23888087e-02  1.50810054e-02  4.29147807e-02\n",
      "   4.33658548e-02  3.76311600e-02  9.04287241e-03  4.70644725e-02\n",
      "   1.00000000e+00  5.14341986e-02  3.33021171e-02  4.00994225e-02\n",
      "   3.11528720e-02 -2.53992944e-04]\n",
      " [ 1.64625992e-02  4.18915226e-02  1.19770206e-02  4.99936211e-02\n",
      "   4.67697353e-02  2.43991608e-02  6.34995511e-02  2.91672639e-02\n",
      "  -1.07850186e-02  1.46155798e-02  3.41039779e-02  3.47968096e-02\n",
      "   4.88964604e-02  2.31927456e-02  6.05218606e-02  2.05511100e-02\n",
      "   5.14341986e-02  1.00000000e+00  5.97218352e-02  2.74858001e-02\n",
      "   2.29902783e-02  2.30764200e-02]\n",
      " [ 2.66983402e-02  5.07778813e-02  2.17407887e-02  2.58202628e-02\n",
      "   5.55360205e-02  4.14951645e-02  6.75080691e-02  5.28106886e-02\n",
      "   5.51542664e-02  5.68873843e-03  1.99567235e-02  3.98763004e-02\n",
      "   1.52287534e-02  1.36387678e-02  1.11587480e-02  3.09067166e-02\n",
      "   3.33021171e-02  5.97218352e-02  1.00000000e+00  2.39474504e-02\n",
      "   1.20627544e-02  3.09397471e-02]\n",
      " [-3.57981629e-03 -1.00154837e-04  4.83161702e-02  1.70302835e-02\n",
      "  -2.92729579e-03  8.02137193e-02  7.14987016e-03  1.96930621e-02\n",
      "   1.21149985e-02  1.96606278e-02  4.25203771e-03  9.53739270e-03\n",
      "   2.97460896e-02  2.42191770e-02  1.52770807e-02  5.74860000e-02\n",
      "   4.00994225e-02  2.74858001e-02  2.39474504e-02  1.00000000e+00\n",
      "  -3.07065471e-02  4.78909831e-02]\n",
      " [ 9.03370597e-03  5.31188130e-03 -8.70886098e-05 -1.13381911e-02\n",
      "   5.47509212e-03  9.86710561e-03  1.51463676e-02 -2.78981964e-03\n",
      "  -6.38882324e-03  5.63884112e-03 -2.01475617e-03  6.59780259e-02\n",
      "  -2.11717157e-02  3.04457647e-02 -4.49526854e-03 -1.12095432e-02\n",
      "   3.11528720e-02  2.29902783e-02  1.20627544e-02 -3.07065471e-02\n",
      "   1.00000000e+00  1.04256946e-02]\n",
      " [ 2.07368315e-02  2.91983717e-02  4.23188694e-02  4.48199899e-02\n",
      "  -1.84536481e-02  2.81362333e-02  7.21636116e-02  1.14867244e-02\n",
      "   2.58360996e-02  1.24286913e-02 -1.77237615e-02 -1.01023007e-02\n",
      "   2.39338538e-03  5.99914448e-03  1.61558954e-02  6.46407448e-04\n",
      "  -2.53992944e-04  2.30764200e-02  3.09397471e-02  4.78909831e-02\n",
      "   1.04256946e-02  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def loadShapedMatrix(mat):\n",
    "    ISC = np.genfromtxt(os.path.join('school_%s.csv' %mat),delimiter=',')\n",
    "    ISC = np.array(ISC,dtype='float') # Convert into array\n",
    "\n",
    "    nParticipants = ISC.shape[0]\n",
    "    nPeriod = int(np.sqrt(ISC.shape[1] / nParticipants))\n",
    "    ISC_sh = np.zeros((ISC.shape[0]*nPeriod,ISC.shape[0]*nPeriod))\n",
    "\n",
    "    nParticipants = ISC.shape[0]\n",
    "    nPeriod = int(np.sqrt(ISC.shape[1] / nParticipants))\n",
    "\n",
    "    ISC_sh = np.zeros((ISC.shape[0]*nPeriod,ISC.shape[0]*nPeriod))\n",
    "\n",
    "    for period1 in range(nPeriod):\n",
    "        for period2 in range(nPeriod):\n",
    "            if (period1 == period2):\n",
    "                subISC_EDA = np.nan_to_num(ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)])\n",
    "                subISC_EDA = (subISC_EDA+subISC_EDA.T)/2\n",
    "            else:\n",
    "                subISC_EDA = ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)]\n",
    "            ISC_sh[nParticipants*period1:nParticipants*(period1+1),nParticipants*period2:nParticipants*(period2+1)] = subISC_EDA\n",
    "    # print(ISC_EDA_sh)\n",
    "    ISC_sh = np.nan_to_num(ISC_sh)\n",
    "    ISC_sh = (ISC_sh+ISC_sh.T)/2\n",
    "\n",
    "    return ISC_sh,nPeriod,nParticipants\n",
    "\n",
    "ISC_EDA,nPeriod,nParticipants = loadShapedMatrix('EDA12')\n",
    "#ISC_IBI,nPeriod,nParticipants = loadShapedMatrix('IBI12')\n",
    "ISC_IBI = ISC_EDA\n",
    "\n",
    "\n",
    "import csv\n",
    "    \n",
    "with open(os.path.join('test.csv'), 'w') as File:\n",
    "    writer = csv.writer(File,delimiter =',')\n",
    "    writer.writerows(ISC_EDA)        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# Remove rows with NaNs\n",
    "def removeNaNSubject(matrix,subject):\n",
    "    new_mat = np.copy(matrix)\n",
    "    new_mat = np.delete(new_mat, (subject), axis=0)\n",
    "    new_mat = np.delete(new_mat, (subject), axis=1)\n",
    "    \n",
    "    '''\n",
    "    with open(os.path.join('school_EDA_truncated.csv'), 'w') as File:\n",
    "        writer = csv.writer(File,delimiter =';')\n",
    "        writer.writerows(matrix)\n",
    "    '''\n",
    "    \n",
    "    return new_mat\n",
    "# Remove outlier in vector if we find one\n",
    "def removeSubjectsVector(vector,subjects):\n",
    "    new_vect = np.copy(vector)\n",
    "    for subj in subjects:\n",
    "        new_vect = np.delete(new_vect, (subj), axis=0)\n",
    "    \n",
    "    return new_vect\n",
    "\n",
    "def removeNaNISC(ISC):\n",
    "\n",
    "    last_i = 0\n",
    "    i = 0\n",
    "    subjectsRemoved_EDA = []\n",
    "    remove = False\n",
    "    nbTotal = len(ISC) \n",
    "    while (last_i < nbTotal and i < nbTotal - 1):\n",
    "        #print(nbTotal)\n",
    "        for i in range(nbTotal):\n",
    "            remove = False\n",
    "            #print(i)\n",
    "            for p in range(nPeriod):\n",
    "                if ISC[p*nParticipants,i] == 0.:\n",
    "                    print(p,nParticipants,i)\n",
    "                    remove = True\n",
    "                    break\n",
    "\n",
    "            if remove:        \n",
    "                ISC = removeNaNSubject(ISC,i)\n",
    "                last_i = i\n",
    "                nbTotal -= 1\n",
    "                #subjectsRemoved_EDA.append(i - nbTotal + 84)\n",
    "                subjectsRemoved_EDA.append(i)\n",
    "                break\n",
    "                \n",
    "    print(subjectsRemoved_EDA)\n",
    "    print(ISC)\n",
    "    return ISC\n",
    "\n",
    "    #print(last_i)\n",
    "\n",
    "ISC_EDA = removeNaNISC(ISC_EDA)\n",
    "    \n",
    "import csv\n",
    "    \n",
    "with open(os.path.join('test.csv'), 'w') as File:\n",
    "    writer = csv.writer(File,delimiter =',')\n",
    "    writer.writerows(ISC_EDA)        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "\n",
    "ISC_EDA = removeNanISC(ISC_EDA)\n",
    "ISC_IBI = removeNanISC(ISC_IBI)\n",
    "ISC_EDA = np.genfromtxt(os.path.join('school_EDA12.csv'),delimiter=',')\n",
    "ISC_EDA = np.array(ISC_EDA,dtype='float') # Convert into array\n",
    "\n",
    "ISC_IBI = np.genfromtxt(os.path.join('school_IBI.csv'),delimiter=',')\n",
    "ISC_IBI = np.array(ISC_IBI,dtype='float') # Convert into array\n",
    "\n",
    "allISC = [ISC_EDA,ISC_IBI]\n",
    "\n",
    "nParticipants = ISC_EDA.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC_EDA.shape[1] / nParticipants))\n",
    "ISC_EDA_sh = np.zeros((ISC_EDA.shape[0]*nPeriod,ISC_EDA.shape[0]*nPeriod))\n",
    "nParticipants = ISC_IBI.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC_IBI.shape[1] / nParticipants))\n",
    "ISC_IBI_sh = np.zeros((ISC_IBI.shape[0]*nPeriod,ISC_IBI.shape[0]*nPeriod))\n",
    "\n",
    "allISC_sh = [ISC_EDA_sh,ISC_IBI_sh]\n",
    "\n",
    "#ISC_IBI = ISC_IBI[0:10,0:10]\n",
    "ISC_IBI = ISC_EDA\n",
    "\n",
    "for i in range(len(allISC)):\n",
    "    \n",
    "    ISC = allISC[i]\n",
    "    ISC_sh = allISC_sh[i]\n",
    "    \n",
    "    nParticipants = ISC.shape[0]\n",
    "    nPeriod = int(np.sqrt(ISC.shape[1] / nParticipants))\n",
    "\n",
    "    ISC_sh = np.zeros((ISC.shape[0]*nPeriod,ISC.shape[0]*nPeriod))\n",
    "\n",
    "    for period1 in range(nPeriod):\n",
    "        for period2 in range(nPeriod):\n",
    "            if (period1 == period2):\n",
    "                subISC_EDA = np.nan_to_num(ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)])\n",
    "                subISC_EDA = (subISC_EDA+subISC_EDA.T)/2\n",
    "            else:\n",
    "                subISC_EDA = ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)]\n",
    "            ISC_sh[nParticipants*period1:nParticipants*(period1+1),nParticipants*period2:nParticipants*(period2+1)] = subISC_EDA\n",
    "\n",
    "    ISC_sh = np.nan_to_num(ISC_sh)\n",
    "    ISC_sh = (ISC_sh+ISC_sh.T)/2\n",
    "\n",
    "import csv\n",
    "    \n",
    "with open(os.path.join('test.csv'), 'w') as File:\n",
    "    writer = csv.writer(File,delimiter =',')\n",
    "    writer.writerows(ISC_EDA)        \n",
    "        \n",
    "        \n",
    "'''\n",
    "\n",
    "for ISC in allISC_sh:\n",
    "    last_i = 0\n",
    "    i = 0\n",
    "    subjectsRemoved_IBI = []\n",
    "    nbTotal = len(ISC)\n",
    "    while (last_i < nbTotal and i < nbTotal - 1):\n",
    "        #print(nbTotal)\n",
    "        for i in range(nbTotal):\n",
    "            #print(i)\n",
    "            if ISC[0,i] == 0.:\n",
    "                ISC = removeNaNSubject(ISC,i)\n",
    "                last_i = i\n",
    "                nbTotal -= 1\n",
    "                #subjectsRemoved_IBI.append(i - nbTotal + 84)\n",
    "                subjectsRemoved_IBI.append(i)\n",
    "                break\n",
    "\n",
    "    print(subjectsRemoved_IBI)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ISC = ISC_EDA\n",
    "nParticipants = ISC.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC.shape[1] / nParticipants))\n",
    "nPeriod\n",
    "ISC_EDA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "0 80 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nfor ISC in allISC_sh:\\n    last_i = 0\\n    i = 0\\n    subjectsRemoved_IBI = []\\n    nbTotal = len(ISC)\\n    while (last_i < nbTotal and i < nbTotal - 1):\\n        #print(nbTotal)\\n        for i in range(nbTotal):\\n            #print(i)\\n            if ISC[0,i] == 0.:\\n                ISC = removeNaNSubject(ISC,i)\\n                last_i = i\\n                nbTotal -= 1\\n                #subjectsRemoved_IBI.append(i - nbTotal + 84)\\n                subjectsRemoved_IBI.append(i)\\n                break\\n\\n    print(subjectsRemoved_IBI)\\n    '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ISC_EDA = np.genfromtxt(os.path.join('school_EDA12.csv'),delimiter=',')\n",
    "ISC_EDA = np.array(ISC_EDA,dtype='float') # Convert into array\n",
    "\n",
    "ISC_IBI = np.genfromtxt(os.path.join('school_IBI.csv'),delimiter=',')\n",
    "ISC_IBI = np.array(ISC_IBI,dtype='float') # Convert into array\n",
    "\n",
    "allISC = [ISC_EDA,ISC_IBI]\n",
    "\n",
    "nParticipants = ISC_EDA.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC_EDA.shape[1] / nParticipants))\n",
    "ISC_EDA_sh = np.zeros((ISC_EDA.shape[0]*nPeriod,ISC_EDA.shape[0]*nPeriod))\n",
    "nParticipants = ISC_IBI.shape[0]\n",
    "nPeriod = int(np.sqrt(ISC_IBI.shape[1] / nParticipants))\n",
    "ISC_IBI_sh = np.zeros((ISC_IBI.shape[0]*nPeriod,ISC_IBI.shape[0]*nPeriod))\n",
    "\n",
    "allISC_sh = [ISC_EDA_sh,ISC_IBI_sh]\n",
    "\n",
    "#ISC_IBI = ISC_IBI[0:10,0:10]\n",
    "ISC_IBI = ISC_EDA\n",
    "\n",
    "for i in range(len(allISC)):\n",
    "    \n",
    "    ISC = allISC[i]\n",
    "    ISC_sh = allISC_sh[i]\n",
    "    \n",
    "    nParticipants = ISC.shape[0]\n",
    "    nPeriod = int(np.sqrt(ISC.shape[1] / nParticipants))\n",
    "\n",
    "    ISC_sh = np.zeros((ISC.shape[0]*nPeriod,ISC.shape[0]*nPeriod))\n",
    "\n",
    "    for period1 in range(nPeriod):\n",
    "        for period2 in range(nPeriod):\n",
    "            if (period1 == period2):\n",
    "                subISC_EDA = np.nan_to_num(ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)])\n",
    "                subISC_EDA = (subISC_EDA+subISC_EDA.T)/2\n",
    "            else:\n",
    "                subISC_EDA = ISC[:,nParticipants*(nPeriod*period1+period2):nParticipants*(nPeriod*period1+period2+1)]\n",
    "            ISC_sh[nParticipants*period1:nParticipants*(period1+1),nParticipants*period2:nParticipants*(period2+1)] = subISC_EDA\n",
    "   # print(ISC_EDA_sh)\n",
    "    ISC_sh = np.nan_to_num(ISC_sh)\n",
    "    ISC_sh = (ISC_sh+ISC_sh.T)/2\n",
    "\n",
    "import csv\n",
    "    \n",
    "with open(os.path.join('test.csv'), 'w') as File:\n",
    "    writer = csv.writer(File,delimiter =',')\n",
    "    writer.writerows(ISC_EDA_sh)        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# Remove rows with NaNs\n",
    "def removeNaNSubject(matrix,subject):\n",
    "    new_mat = np.copy(matrix)\n",
    "    new_mat = np.delete(new_mat, (subject), axis=0)\n",
    "    new_mat = np.delete(new_mat, (subject), axis=1)\n",
    "    \n",
    "    '''\n",
    "    with open(os.path.join('school_EDA_truncated.csv'), 'w') as File:\n",
    "        writer = csv.writer(File,delimiter =';')\n",
    "        writer.writerows(matrix)\n",
    "    '''\n",
    "    \n",
    "    return new_mat\n",
    "# Remove outlier in vector if we find one\n",
    "def removeSubjectsVector(vector,subjects):\n",
    "    new_vect = np.copy(vector)\n",
    "    for subj in subjects:\n",
    "        new_vect = np.delete(new_vect, (subj), axis=0)\n",
    "    \n",
    "    return new_vect\n",
    "\n",
    "for ISC in allISC_sh:\n",
    "    print(ISC)\n",
    "    last_i = 0\n",
    "    i = 0\n",
    "    subjectsRemoved_EDA = []\n",
    "    remove = False\n",
    "    nbTotal = len(ISC) \n",
    "    while (last_i < nbTotal and i < nbTotal - 1):\n",
    "        #print(nbTotal)\n",
    "        for i in range(nbTotal):\n",
    "            remove = False\n",
    "            #print(i)\n",
    "            for p in range(nPeriod):\n",
    "                if ISC[p*nParticipants,i] == 0.:\n",
    "                    print(p,nParticipants,i)\n",
    "                    remove = True\n",
    "                    break\n",
    "\n",
    "            if remove:        \n",
    "                ISC = removeNaNSubject(ISC,i)\n",
    "                last_i = i\n",
    "                nbTotal -= 1\n",
    "                #subjectsRemoved_EDA.append(i - nbTotal + 84)\n",
    "                subjectsRemoved_EDA.append(i)\n",
    "                break\n",
    "                \n",
    "    print(subjectsRemoved_EDA)\n",
    "    print(ISC)\n",
    "\n",
    "    #print(last_i)\n",
    "    \n",
    "#print(ISC_EDA[:4,:4])\n",
    "'''\n",
    "\n",
    "for ISC in allISC_sh:\n",
    "    last_i = 0\n",
    "    i = 0\n",
    "    subjectsRemoved_IBI = []\n",
    "    nbTotal = len(ISC)\n",
    "    while (last_i < nbTotal and i < nbTotal - 1):\n",
    "        #print(nbTotal)\n",
    "        for i in range(nbTotal):\n",
    "            #print(i)\n",
    "            if ISC[0,i] == 0.:\n",
    "                ISC = removeNaNSubject(ISC,i)\n",
    "                last_i = i\n",
    "                nbTotal -= 1\n",
    "                #subjectsRemoved_IBI.append(i - nbTotal + 84)\n",
    "                subjectsRemoved_IBI.append(i)\n",
    "                break\n",
    "\n",
    "    print(subjectsRemoved_IBI)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDistanceMatrix(study_matrix):\n",
    "    \n",
    "    # Normalise maximum value to 1\n",
    "    normed_matrix = np.copy(study_matrix)\n",
    "    normed_matrix = normed_matrix / np.max(abs(normed_matrix))\n",
    "    \n",
    "    # Convert into distance matrix\n",
    "    distance_matrix = np.sqrt((1-normed_matrix)) # Formula in Matlab and in Scikit to convert \n",
    "        \n",
    "    # Interval MDS normalization to spread values between 0 and 1\n",
    "    a = np.min(np.sort(distance_matrix,axis=0)[1,:])\n",
    "    distance_matrix = distance_matrix - a\n",
    "    distance_matrix = distance_matrix - np.diag(np.diag(distance_matrix))\n",
    "    distance_matrix = distance_matrix / np.max(distance_matrix)\n",
    "    \n",
    "    # To make sure output is perfectly symmetrical\n",
    "    return (distance_matrix+distance_matrix.T)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiview mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAllDistanceMatrix(ISC_EDA,ISC_IBI):\n",
    "    \n",
    "    # Make copy of 2 matrices (for each modality)\n",
    "    mat_EDA = np.copy(ISC_EDA)\n",
    "    mat_IBI = np.copy(ISC_IBI)\n",
    "    \n",
    "    # Compute the 2 distance matrices (according to each modality)\n",
    "    distance_matrix_EDA = computeDistanceMatrix(ISC_EDA)\n",
    "    distance_matrix_IBI = computeDistanceMatrix(ISC_IBI)\n",
    "  \n",
    "    # Remove direct correlation influence and compute correlation distance matrices\n",
    "    for i in range(len(mat_EDA)):\n",
    "        mat_EDA[i,i] = 0\n",
    "    for i in range(len(mat_IBI)):\n",
    "        mat_IBI[i,i] = 0\n",
    "    distance_matrix_corr_EDA = pairwise_distances(mat_EDA,metric='correlation')\n",
    "    distance_matrix_corr_IBI = pairwise_distances(mat_IBI,metric='correlation')\n",
    "\n",
    "    # Return distance matrices\n",
    "    return distance_matrix_EDA, distance_matrix_IBI, distance_matrix_corr_EDA, distance_matrix_corr_IBI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def computeAllDistanceMatrix(ISC_EDA,ISC_IBI):\n",
    "    \n",
    "    # Make copy of 2 matrices (for each modality)\n",
    "    mat_EDA = np.copy(ISC_EDA)\n",
    "    \n",
    "    # Compute the 2 distance matrices (according to each modality)\n",
    "    distance_matrix_EDA = computeDistanceMatrix(ISC_EDA)\n",
    "  \n",
    "    # Remove direct correlation influence and compute correlation distance matrices\n",
    "    for i in range(len(ISC_EDA)):\n",
    "        mat_EDA[i,i] = 0\n",
    "    distance_matrix_corr_EDA = pairwise_distances(mat_EDA,metric='correlation')\n",
    "\n",
    "    # Return distance matrices\n",
    "    return distance_matrix_EDA, distance_matrix_corr_EDA\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCoordinate(mat=['EDA','IBI','corrEDA','corrIBI'],added=False):\n",
    "\n",
    "    # Compute all distance matrices\n",
    "    distance_matrix_EDA, distance_matrix_IBI, distance_matrix_corr_EDA, distance_matrix_corr_IBI = computeAllDistanceMatrix(ISC_EDA,ISC_IBI)    \n",
    "    #distance_matrix_EDA, distance_matrix_corr_EDA = computeAllDistanceMatrix(ISC_EDA,ISC_IBI)    \n",
    "\n",
    "    # Choose which matrices to include into the multiview\n",
    "    multiviewMat = []\n",
    "    if ('EDA' in mat):\n",
    "        multiviewMat.append(distance_matrix_EDA)\n",
    "        distance_matrix = distance_matrix_EDA\n",
    "    if ('IBI' in mat):\n",
    "        multiviewMat.append(distance_matrix_IBI)\n",
    "        distance_matrix = distance_matrix_IBI\n",
    "    if ('corrEDA' in mat):\n",
    "        multiviewMat.append(distance_matrix_corr_EDA)\n",
    "    if ('corrIBI' in mat):\n",
    "        multiviewMat.append(distance_matrix_corr_IBI)\n",
    "        \n",
    "    # Compute mapping\n",
    "    embeddingDim = dim # to have a 2D map\n",
    "    \n",
    "    if len(mat)==1:\n",
    "        points = np.array(pcoa(distance_matrix, method='eigh', number_of_dimensions=embeddingDim).samples)\n",
    "        print(\"pcoa proportion explained : %s \" %np.sum(pcoa(distance_matrix, method='eigh', number_of_dimensions=embeddingDim).proportion_explained))\n",
    "    else:\n",
    "        points = mvmds(multiviewMat,len(mat)*[True],embeddingDim,added=added)\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def computeUMAPCoordinate(mat,sslLabel=None,n_neighbors=3):\n",
    "\n",
    "    if (mat==['EDA']):\n",
    "        study_matrix = ISC_EDA\n",
    "    if (mat==['IBI']):\n",
    "        study_matrix = ISC_IBI\n",
    "\n",
    "    print(dim)\n",
    "    print(type(dim))\n",
    "    fitter = umap.UMAP(n_components=int(dim),n_neighbors=n_neighbors,metric='correlation',min_dist=0.0,init='spectral',target_weight=0.5,n_epochs=5000).fit(study_matrix,sslLabel)\n",
    "    #points = preprocessing.scale(fitter.embedding_,axis=0)\n",
    "    points = (fitter.embedding_ - np.mean(fitter.embedding_,axis=0)) / np.std(fitter.embedding_,axis=0)\n",
    "    #print(np.std(points))\n",
    "    #print(np.mean(points))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def computeSCCoordinate(mat,gamma=1):\n",
    "    \n",
    "    distance_matrix_EDA, distance_matrix_IBI, distance_matrix_corr_EDA, distance_matrix_corr_IBI = computeAllDistanceMatrix(ISC_EDA,ISC_IBI)    \n",
    "    #distance_matrix_EDA, distance_matrix_corr_EDA = computeAllDistanceMatrix(ISC_EDA,ISC_IBI)    \n",
    "\n",
    "    if (mat==['EDA']):\n",
    "        distance_matrix = distance_matrix_EDA\n",
    "    if (mat==['IBI']):\n",
    "        distance_matrix = distance_matrix_IBI\n",
    "    \n",
    "    \n",
    "    clusteringDist = SpectralClustering(n_clusters=dim+1, n_init=10,affinity='precomputed')\n",
    "    affinity_matrix = np.exp(-gamma * distance_matrix ** 2)\n",
    "    clusteringDist.fit(affinity_matrix)\n",
    "    return clusteringDist.map_[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def computeNoneCoordinate(mat):\n",
    "    \n",
    "    distance_matrix_EDA, distance_matrix_IBI, distance_matrix_corr_EDA, distance_matrix_corr_IBI = computeAllDistanceMatrix(ISC_EDA,ISC_IBI)    \n",
    "    #distance_matrix_EDA, distance_matrix_corr_EDA = computeAllDistanceMatrix(ISC_EDA,ISC_IBI)    \n",
    "\n",
    "    if (mat==['EDA']):\n",
    "        distance_matrix = distance_matrix_EDA\n",
    "    if (mat==['IBI']):\n",
    "        distance_matrix = distance_matrix_IBI\n",
    "    \n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results():\n",
    "\n",
    "    def __init__(self,mat,mapping,algo):\n",
    "        self.mat = mat\n",
    "        self.mapping = mapping\n",
    "        self.algo = algo\n",
    "        \n",
    "    def twoClustersMethodResult(self):\n",
    "        print(\"Blind results\\n\")\n",
    "        \n",
    "        # Cluster tendency\n",
    "        print(\"Hopkins test\")\n",
    "        self.hopkinsCoef()\n",
    "        print(\"%f +- %f\" %(self.hopkins[0],self.hopkins[1]))\n",
    "        \n",
    "        # Found clusters\n",
    "        print(\"Found clusters \")\n",
    "        print(self.label)\n",
    "        \n",
    "        # Clustering quality evaluation\n",
    "        print(\"Silhouette coefficient : %0.3f\" %self.silhouetteCoef())\n",
    "        print(\"DB index : %f \" %self.dbScore())\n",
    "        \n",
    "        # Comparing to ground truth (if known)\n",
    "        if (os.path.isfile(os.path.join('conditionSchool_EDA.csv')) and os.path.isfile(os.path.join('conditionSchool_IBI.csv'))):\n",
    "            global groundTruth\n",
    "            if (groundTruth):\n",
    "                print(\"\\n\\nComparing to ground truth\\n\")\n",
    "                print(\"Accuracy : %f \" %self.accuracy())\n",
    "                #print(\"Well participants : [%s]\" % \", \".join(map(str, self.wellClassified)))        \n",
    "                print(\"Misclassified participants : [%s]\" % \", \".join(map(str, self.misClassified)))        \n",
    "\n",
    "    def applyMethod(self,method):\n",
    "        \n",
    "        # Mapping\n",
    "        if (mapping == 'UMAP'):\n",
    "            points = computeUMAPCoordinate(self.mat)\n",
    "            #print(points)\n",
    "        elif (mapping == 'MDS'):\n",
    "            points = computeCoordinate(self.mat,added=False)\n",
    "        elif (mapping == 'MDS_scale'):\n",
    "            points = computeCoordinate(self.mat,added=True)\n",
    "        elif (mapping == 'SC'):\n",
    "            points = computeSCCoordinate(self.mat)\n",
    "        elif (mapping == 'None'): # distance matrix based clustering algorithm\n",
    "            print(\"dist meth\")\n",
    "            points = computeNoneCoordinate(self.mat)\n",
    "            if (algo == 'Spectral Clustering distance'):\n",
    "                points = np.exp(-method.gamma * points ** 2)\n",
    "        \n",
    "        # Clustering\n",
    "        # if (meth.distance_matrix):\n",
    "        if (algo == 'K-Medoids'):\n",
    "            meth = method.fit(pairwise_distances(points))\n",
    "        else:\n",
    "            meth = method.fit(points)\n",
    "\n",
    "        # Store found labels and points location\n",
    "        if hasattr(meth, 'labels_'):\n",
    "            label = meth.labels_\n",
    "        else:\n",
    "            label = meth.predict(points)\n",
    "            \n",
    "        self.label = label\n",
    "        self.best_points = points\n",
    "         \n",
    "    def showResultMap(self):\n",
    "        \n",
    "        # Set new figure\n",
    "        plt.figure()\n",
    "        ax = plt.axes([0,0,1.2,1.2])\n",
    "        ax.set_aspect(aspect='equal')\n",
    "        \n",
    "        # Annotate points\n",
    "        if mat == ['EDA']:\n",
    "            n = len(ISC_EDA)\n",
    "        elif mat == ['IBI']:\n",
    "            n = len(ISC_IBI)        \n",
    "        for i in range(n):\n",
    "            ax.annotate(i,(self.best_points[i,0],self.best_points[i,1]),xytext=(self.best_points[i,0]+(np.max(self.best_points[:,0])-np.min(self.best_points[:,0]))/50,self.best_points[i,1]))     \n",
    "    \n",
    "        # Show color according to trueDisplay bool\n",
    "        global trueDisplay\n",
    "        if (trueDisplay):\n",
    "            # Scatter points\n",
    "            for i in range(n_clusters):\n",
    "                ax.scatter(self.best_points[self.trueGroups[i],0],self.best_points[self.trueGroups[i],1],s=145,label=str(i))\n",
    "            \n",
    "            # Add legend\n",
    "            TP = mpatches.Patch(color='blue', label='NA')\n",
    "            TN = mpatches.Patch(color='red', label='SSA')\n",
    "            FP_FN = mpatches.Patch(color='black', label='misclassified')\n",
    "            #plt.legend(handles=[TP,TN,FP_FN])\n",
    "            plt.title(\"%s on [%s]\" % (self.algo, \", \".join(map(str, self.mat))))\n",
    "            #plt.axis('off')\n",
    "            #axes = plt.gca()\n",
    "            #axes.set_xlim([-0.4,0.4])\n",
    "            #axes.set_ylim([-0.4,0.7])\n",
    "            plt.savefig(os.path.join('figures','GT_%s.png' %self.mat),bbox_inches='tight')\n",
    "        else:\n",
    "            # Scatter points\n",
    "            for i in range(n_clusters):\n",
    "                ax.scatter(self.best_points[self.label == i,0],self.best_points[self.label == i,1],s=145,label=str(i))\n",
    "            \n",
    "            # Add legend\n",
    "            group0 = mpatches.Patch(color='darkgreen', label='First group')\n",
    "            group1 = mpatches.Patch(color='darkorange', label='Second group')\n",
    "            #plt.legend(handles=[group0,group1])       \n",
    "            plt.title(\"%s on [%s]\" % (self.algo, \", \".join(map(str, self.mat))))\n",
    "            #plt.axis('off')\n",
    "            plt.savefig(os.path.join('figures','clustering_%s.png' %self.mat),bbox_inches='tight')\n",
    "\n",
    "        \n",
    "    def dbScore(self):\n",
    "        # Compute DB-score (small means good clustering)\n",
    "        if len(np.unique(self.label))==1:\n",
    "            warnings.warn(\"Labels correspond to only 1 group\")\n",
    "            return -1\n",
    "        elif len(np.unique(self.label))==len(ISC_EDA):\n",
    "            warnings.warn(\"Each participant correspond to one label\")\n",
    "            return -1\n",
    "        elif len(np.unique(self.label))==len(ISC_IBI):\n",
    "            warnings.warn(\"Each participant correspond to one label\")\n",
    "            return -1\n",
    "        else:\n",
    "            return davies_bouldin_score(self.best_points,self.label)\n",
    "        \n",
    "    def hopkinsCoef(self):\n",
    "        # Average on different random data generated in Hopkins\n",
    "        H = []\n",
    "        if mat == ['EDA']:\n",
    "            n = len(ISC_EDA)\n",
    "        elif mat == ['IBI']:\n",
    "            n = len(ISC_IBI)\n",
    "        for p in range(100):\n",
    "            H.append(hopkins(self.best_points,n)) \n",
    "        self.hopkins = [1-np.mean(H),np.std(H)]\n",
    "        \n",
    "        # Raise a warning if Hopkins test <= 0.5\n",
    "        if (self.hopkins[0]<=0.5):\n",
    "            warnings.warn(\"Hopkins test <= 0.5 : data set does not have clustering tendency\")\n",
    "            \n",
    "    def silhouetteCoef(self):\n",
    "        # Compute Silhouette Coefficient based on distance matrix (the closer it is to 1, the better the clustering is)\n",
    "        if len(np.unique(self.label))==1:\n",
    "            warnings.warn(\"Labels correspond to only 1 group\")\n",
    "            return -1\n",
    "        elif len(np.unique(self.label))==len(ISC_EDA):\n",
    "            warnings.warn(\"Each participant correspond to one label\")\n",
    "            return -1\n",
    "        elif len(np.unique(self.label))==len(ISC_IBI):\n",
    "            warnings.warn(\"Each participant correspond to one label\")\n",
    "            return -1\n",
    "        else:\n",
    "            return silhouette_score(self.best_points, self.label, metric='euclidean')\n",
    "        \n",
    "    def accuracy(self):\n",
    "        # Read true labels from CSV file\n",
    "        condition = np.genfromtxt(os.path.join('conditionSchool_%s.csv' %mat[0]),delimiter=',')\n",
    "        condition = np.array(condition,dtype='int') # Convert into array\n",
    "        if mat == ['EDA']:\n",
    "            condition = removeSubjectsVector(condition,subjectsRemoved_EDA)\n",
    "        elif mat == ['IBI']:\n",
    "            condition = removeSubjectsVector(condition,subjectsRemoved_IBI)\n",
    "        self.trueGroups = []\n",
    "        for i in range(n_clusters):\n",
    "            self.trueGroups.append(np.where(condition == i)[0])\n",
    "        \n",
    "        # Find which participants are misclassified and compute accuracy\n",
    "        if mat == ['EDA']:\n",
    "            subjects = np.arange(len(ISC_EDA))\n",
    "        elif mat == ['IBI']:\n",
    "            subjects = np.arange(len(ISC_IBI))\n",
    "        self.wellClassified = subjects[self.label==condition]\n",
    "        self.misClassified = subjects[self.label!=condition]\n",
    "        '''if (len(self.misClassified) > len(self.wellClassified)):\n",
    "            self.wellClassified, self.misClassified = self.misClassified, self.wellClassified\n",
    "        \n",
    "        return len(self.wellClassified) / len(ISC_EDA)\n",
    "        '''\n",
    "        print(len(ISC_IBI))\n",
    "        print(condition)\n",
    "        print(self.label)\n",
    "        return accuracy(condition,self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which matrix or matrices to study\n",
    "mat = ['IBI']\n",
    "#mat = ['EDA','IBI']\n",
    "\n",
    "# Decide if compare to ground truth\n",
    "groundTruth = True\n",
    "\n",
    "# Display found clusters or true clusters with misclassified participants in black\n",
    "trueDisplay = True\n",
    "\n",
    "# Choose mapping method\n",
    "#mapping = 'UMAP'\n",
    "#mapping = 'MDS'\n",
    "#mapping = 'MDS_scale'\n",
    "mapping = 'SC'\n",
    "#mapping = 'None'\n",
    "\n",
    "# Choose clustering algorithm\n",
    "algo = 'K-Means'\n",
    "#algo = 'Spectral Clustering'\n",
    "#algo = 'Hierarchical Clustering'\n",
    "#algo = 'K-Medoids'\n",
    "\n",
    "#algo = 'Spectral Clustering distance'\n",
    "#algo = 'Hierarchical Clustering distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute chosen clustering\n",
    "method_result = Results(mat=mat,mapping=mapping,algo=algo)\n",
    "n_clusters = 2\n",
    "dim = 2\n",
    "\n",
    "if (algo == 'K-Means'):\n",
    "    method = KMeans(n_clusters=n_clusters,n_init=100)\n",
    "elif (algo == 'Spectral Clustering'):\n",
    "    method = SpectralClustering(n_clusters=n_clusters,n_init=10,gamma=1)\n",
    "elif (algo == 'K-Medoids'):\n",
    "    method = KMedoids(n_clusters=n_clusters,tmax=1000)\n",
    "elif (algo == 'Hierarchical Clustering'):\n",
    "    method = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"ward\")\n",
    "    \n",
    "elif (algo == 'Spectral Clustering distance'):\n",
    "    method = SpectralClustering(n_clusters=n_clusters,n_init=10,gamma=1,affinity='precomputed')\n",
    "elif (algo == 'Hierarchical Clustering distance'):\n",
    "    method = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"single\",affinity='precomputed')\n",
    "#elif (algo == 'Hierarchical Clustering distance'): # ward doesnt work with precomputed matrix\n",
    "#    method = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"ward\",affinity='precomputed')\n",
    "        \n",
    "method_result.applyMethod(method)\n",
    "\n",
    "# Show results\n",
    "method_result.twoClustersMethodResult()\n",
    "method_result.showResultMap()\n",
    "\n",
    "plt.figure()\n",
    "groundTruth = False\n",
    "trueDisplay = False\n",
    "method_result.showResultMap()\n",
    "\n",
    "from sklearn.metrics import homogeneity_score\n",
    "condition = np.genfromtxt(os.path.join('conditionSchool_%s.csv' %mat[0]),delimiter=',')\n",
    "condition = np.array(condition,dtype='int') # Convert into array\n",
    "if mat == ['EDA']:\n",
    "    condition = removeSubjectsVector(condition,subjectsRemoved_EDA)\n",
    "elif mat == ['IBI']:\n",
    "    condition = removeSubjectsVector(condition,subjectsRemoved_IBI)\n",
    "print(\"homogeneity\")\n",
    "print(homogeneity_score(condition,method_result.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute chosen clustering\n",
    "method_result = Results(mat=mat,mapping=mapping,algo=algo)\n",
    "n_clusters = 17\n",
    "accDim = []\n",
    "allDim = np.arange(2,30)\n",
    "\n",
    "if (algo == 'K-Means'):\n",
    "    method = KMeans(n_clusters=n_clusters,n_init=100)\n",
    "elif (algo == 'Spectral Clustering'):\n",
    "    method = SpectralClustering(n_clusters=n_clusters,n_init=10,gamma=1)\n",
    "elif (algo == 'K-Medoids'):\n",
    "    method = KMedoids(n_clusters=n_clusters,tmax=1000)\n",
    "elif (algo == 'Hierarchical Clustering'):\n",
    "    method = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"ward\")\n",
    "\n",
    "for dim in allDim:\n",
    "    method_result.applyMethod(method)\n",
    "    accDim.append(method_result.accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(allDim,accDim)\n",
    "plt.xlabel(\"number of dimensions\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test rendering with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.random.rand(100, 5),\n",
    "    columns=[\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    ")\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report')\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(24)\n",
    "df = pd.DataFrame({'A': np.linspace(1, 10, 10)})\n",
    "df = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\n",
    "               axis=1)\n",
    "df.iloc[3, 3] = np.nan\n",
    "df.iloc[0, 2] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaNs\n",
    "def removeNaNSubject(matrix,subject,period1,period2):\n",
    "    new_mat = np.copy(matrix)\n",
    "    new_mat = np.delete(new_mat[:,:,period1,period2], (subject), axis=0)\n",
    "    new_mat = np.delete(new_mat[:,:,period1,period2], (subject), axis=1)\n",
    "    \n",
    "    '''\n",
    "    with open(os.path.join('school_EDA_truncated.csv'), 'w') as File:\n",
    "        writer = csv.writer(File,delimiter =';')\n",
    "        writer.writerows(matrix)\n",
    "    '''\n",
    "    \n",
    "    return new_mat\n",
    "# Remove outlier in vector if we find one\n",
    "def removeSubjectsVector(vector,subjects):\n",
    "    new_vect = np.copy(vector)\n",
    "    for subj in subjects:\n",
    "        new_vect = np.delete(new_vect, (subj), axis=0)\n",
    "    \n",
    "    return new_vect\n",
    "'''\n",
    "last_i = 0\n",
    "i = 0\n",
    "subjectsRemoved_EDA = []\n",
    "nbTotal = ISC_EDA.shape[0]\n",
    "while (last_i < nbTotal and i < nbTotal - 1):\n",
    "    #print(nbTotal)\n",
    "    for i in range(nbTotal):\n",
    "        #print(i)\n",
    "        if ISC_EDA[0,i] == 0.:\n",
    "            ISC_EDA = removeNaNSubject(ISC_EDA,i)\n",
    "            last_i = i\n",
    "            nbTotal -= 1\n",
    "            #subjectsRemoved_EDA.append(i - nbTotal + 84)\n",
    "            subjectsRemoved_EDA.append(i)\n",
    "            break\n",
    "\n",
    "    #print(last_i)\n",
    "    \n",
    "print(ISC_EDA[:4,:4])\n",
    "print(subjectsRemoved_EDA)\n",
    "'''\n",
    "\n",
    "last_i = 0\n",
    "i = 0\n",
    "subjectsRemoved_IBI = []\n",
    "nbTotal = ISC_IBI.shape[0]\n",
    "print(ISC_IBI.shape)\n",
    "while (last_i < nbTotal and i < nbTotal - 1):\n",
    "    #print(nbTotal)\n",
    "    for i in range(nbTotal):\n",
    "        for period1 in range(nPeriod):\n",
    "            for period2 in range(nPeriod):\n",
    "                #print(i,period1,period2)\n",
    "                if ISC_IBI[0,i,period1,period2] == 0.:\n",
    "                    ISC_IBI = removeNaNSubject(ISC_IBI,i,period1,period2)\n",
    "                    last_i = i\n",
    "                    nbTotal -= 1\n",
    "                    #subjectsRemoved_IBI.append(i - nbTotal + 84)\n",
    "                    subjectsRemoved_IBI.append(i)\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
